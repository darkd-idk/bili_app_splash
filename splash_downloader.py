#!/usr/bin/env python3
"""
Bilibili Splash Image Downloader (Complete Solution with Enhanced API Handling)

Version: 5.0.0
Updated: 2025-06-15
"""

import argparse
import hashlib
import json
import logging
import os
import sys
import time
import requests
from datetime import datetime
from pathlib import Path
import random
import traceback

# é…ç½®æ ¹æ—¥å¿—å™¨
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# æ§åˆ¶å°æ—¥å¿—å¤„ç†
console_handler = logging.StreamHandler()
log_formatter = logging.Formatter(
    '[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(log_formatter)
root_logger.addHandler(console_handler)

logger = logging.getLogger("BilibiliSplash")

# API ç«¯ç‚¹é…ç½®
SPLASH_API = "https://app.bilibili.com/x/v2/splash/show"
ALTERNATE_API = "https://api.bilibili.com/x/v2/splash/list"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
HEADERS = {
    "User-Agent": USER_AGENT,
    "Referer": "https://www.bilibili.com/",
    "Origin": "https://www.bilibili.com",
    "Accept": "application/json, text/plain, */*",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
    "Sec-Fetch-Dest": "empty",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-site",
}
DEFAULT_PARAMS = {
    "appkey": "1d8b6e7d45233436",
    "build": "100000",
    "platform": "android",
    "device": "phone",
    "channel": "xiaomi"
}
PROXIES = {
    "http": "socks5h://your-proxy:1080",
    "https": "socks5h://your-proxy:1080"
}

class SplashDownloader:
    def __init__(self, output_dir="splash", url_file="splash_urls.txt", log_file="splash.log", use_proxy=False):
        # è§£æè¾“å‡ºç›®å½•ä¸ºç»å¯¹è·¯å¾„
        self.output_dir = Path(output_dir).resolve()
        self.url_file = Path(url_file).resolve()
        self.log_file = Path(log_file).resolve()
        self.use_proxy = use_proxy
        
        # ç¡®ä¿æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.url_file.touch(exist_ok=True)
        self.log_file.touch(exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡ä»¶æ—¥å¿—
        self.setup_file_logger()
        
        # åˆå§‹åŒ–è®¡æ•°å™¨
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0
        self.start_time = time.time()
        
        # åŠ è½½URLåˆ—è¡¨
        self.url_list = self.load_url_list()
        
        logger.info("=" * 70)
        logger.info(f"âš™ï¸ Splash Downloader Initialized")
        logger.info(f"ğŸ“ Output Directory: {self.output_dir}")
        logger.info(f"ğŸ“ URL File: {self.url_file} ({len(self.url_list)} URLs)")
        logger.info(f"ğŸ“‹ Log File: {self.log_file}")
        logger.info(f"ğŸŒ API Endpoint: {SPLASH_API}")
        logger.info(f"ğŸ”Œ Proxy Enabled: {use_proxy}")
        logger.info("=" * 70)
    
    def setup_file_logger(self):
        """è®¾ç½®æ–‡ä»¶æ—¥å¿—è®°å½•å™¨"""
        file_handler = logging.FileHandler(self.log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(log_formatter)
        file_handler.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        
        # è®°å½•åˆå§‹åŒ–ä¿¡æ¯
        logging.info("\n" + "=" * 70)
        logging.info(f"{' Bilibili Splash Downloader ':^70}")
        logging.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S'):^70}")
        logging.info("=" * 70)
    
    def load_url_list(self):
        """åŠ è½½å·²å¤„ç†çš„URLåˆ—è¡¨"""
        url_set = set()
        try:
            if self.url_file.is_file() and self.url_file.stat().st_size > 0:
                with open(self.url_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            # æ ¼å¼ï¼šhash|url æˆ– url
                            parts = line.split('|')
                            if len(parts) == 2:
                                url_set.add(parts[1])  # æå–URL
                            else:
                                url_set.add(line)
                logger.info(f"Loaded {len(url_set)} URLs from {self.url_file}")
        except Exception as e:
            logger.error(f"Failed to load URL list: {str(e)}")
        return url_set
    
    def save_url_to_list(self, content_hash, url):
        """å°†URLä¿å­˜åˆ°æ–‡ä»¶"""
        with open(self.url_file, 'a', encoding='utf-8') as f:
            f.write(f"{content_hash}|{url}\n")
    
    def request_api(self, api_url, params):
        """å‘é€APIè¯·æ±‚ï¼Œæ”¯æŒé‡è¯•å’Œä»£ç†"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…é™æµ
                time.sleep(random.uniform(0.5, 1.5))
                
                request_headers = {**HEADERS}
                request_params = {**DEFAULT_PARAMS, **params}
                
                # è®°å½•è¯·æ±‚è¯¦æƒ…
                logger.info(f"ğŸŒ Request attempt {attempt+1}/{max_retries}")
                logger.debug(f"Request URL: {api_url}")
                logger.debug(f"Request Headers: {json.dumps(request_headers, indent=2)}")
                logger.debug(f"Request Params: {json.dumps(request_params, indent=2)}")
                
                # ä½¿ç”¨ä»£ç†
                proxies = PROXIES if self.use_proxy else None
                verify_ssl = not self.use_proxy  # ä½¿ç”¨ä»£ç†æ—¶ä¸éªŒè¯SSL
                
                response = requests.get(
                    api_url,
                    headers=request_headers,
                    params=request_params,
                    timeout=30,
                    proxies=proxies,
                    verify=verify_ssl
                )
                
                # è®°å½•å“åº”è¯¦æƒ…
                logger.info(f"ğŸ“¡ Received response: {response.status_code}, size: {len(response.content)} bytes")
                logger.debug(f"Response Headers: {response.headers}")
                
                # ä¿å­˜åŸå§‹å“åº”
                with open("api_response.json", "w", encoding="utf-8") as f:
                    json.dump({"url": api_url, "params": request_params, "response": response.text}, f, ensure_ascii=False, indent=2)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                if response.status_code != 200:
                    logger.warning(f"API returned non-200 status: {response.status_code}")
                    continue  # é‡è¯•
                
                try:
                    data = response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse JSON response: {str(e)}")
                    logger.debug(f"Response content: {response.text[:500]}")
                    continue  # é‡è¯•
                
                return data
            except requests.RequestException as e:
                logger.warning(f"Request failed: {str(e)}, retrying...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                break
        
        logger.error(f"API request failed after {max_retries} attempts")
        return None
    
    def fetch_splash_list(self):
        """è·å–å¼€å±å›¾åˆ—è¡¨ï¼Œæ”¯æŒä¸»å¤‡API"""
        # å°è¯•ä¸»API
        api_urls = [
            {"url": SPLASH_API, "params": {"plat": 0}},
            {"url": ALTERNATE_API, "params": {"plat": 1}},
            {"url": SPLASH_API, "params": {"plat": 2}}  # å°è¯•ä¸åŒå¹³å°
        ]
        
        # éšæœºæ’åºAPIåˆ—è¡¨
        random.shuffle(api_urls)
        
        for api in api_urls:
            data = self.request_api(api["url"], api["params"])
            if not data:
                continue
            
            # æ£€æŸ¥APIé”™è¯¯ä»£ç 
            if data.get('code') != 0:
                error_msg = data.get('message', 'Unknown error')
                logger.warning(f"API error [{data.get('code')}]: {error_msg}")
                continue
            
            # å°è¯•ä¸åŒæ•°æ®è·¯å¾„
            data_paths = [
                data.get('data', {}).get('list', []),
                data.get('data', []),
                data.get('list', [])
            ]
            
            for path in data_paths:
                if isinstance(path, list) and path:
                    logger.info(f"ğŸ“š Found {len(path)} splash items at API: {api['url']}")
                    return path
            
            logger.debug(f"API response data paths: {json.dumps(data, indent=2)}")
        
        logger.error("All API endpoints failed to return valid splash items")
        return None
    
    def download_image(self, url, metadata=None):
        """ä¸‹è½½å•ä¸ªå¼€å±å›¾ - ä½¿ç”¨å†…å®¹å“ˆå¸Œé¿å…é‡å¤"""
        if not url or len(url) < 10:
            logger.warning("Skipping invalid URL")
            return None
            
        # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†
        if url in self.url_list:
            self.skipped_count += 1
            logger.info(f"â© URL already processed: {url}")
            return None
            
        # ä¸‹è½½å›¾åƒ
        try:
            logger.info(f"â¬‡ï¸ Downloading: {url}")
            # ä½¿ç”¨ä»£ç†è®¾ç½®
            proxies = PROXIES if self.use_proxy else None
            verify_ssl = not self.use_proxy
            
            response = requests.get(
                url, 
                headers=HEADERS,
                timeout=30, 
                stream=True,
                proxies=proxies,
                verify=verify_ssl
            )
            response.raise_for_status()
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            hash_sha256 = hashlib.sha256()
            content = b''
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    hash_sha256.update(chunk)
                    content += chunk
                    
            content_hash = hash_sha256.hexdigest()
            logger.debug(f"Content SHA256: {content_hash}")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            existing_file = None
            for file in self.output_dir.iterdir():
                if file.is_file() and file.stem.startswith(content_hash):
                    existing_file = file
                    break
            
            # å¦‚æœç›¸åŒå†…å®¹å·²å­˜åœ¨
            if existing_file:
                logger.info(f"ğŸ¯ Identical content found: {existing_file.name}")
                self.skipped_count += 1
                # è®°å½•URLä»¥é˜²ä¸‹æ¬¡é‡æ–°ä¸‹è½½
                self.url_list.add(url)
                self.save_url_to_list(content_hash, url)
                return existing_file
                
            # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            filename = f"{content_hash}_{timestamp}_splash.jpg"
            save_path = self.output_dir / filename
            
            # ä¿å­˜æ–‡ä»¶
            with open(save_path, 'wb') as f:
                f.write(content)
            
            # è®°å½•URL
            self.url_list.add(url)
            self.save_url_to_list(content_hash, url)
            
            self.downloaded_count += 1
            file_size = save_path.stat().st_size // 1024
            logger.info(f"âœ… Downloaded: {save_path.name} ({file_size} KB)")
            
            return save_path
        except Exception as e:
            self.failed_count += 1
            logger.error(f"âŒ Failed to download {url}: {str(e)}")
            logger.debug(traceback.format_exc())
            return None
    
    def run(self):
        """æ‰§è¡Œä¸‹è½½è¿‡ç¨‹"""
        logger.info("ğŸš€ Starting splash image download")
        success = False
        
        try:
            splash_list = self.fetch_splash_list()
            
            if splash_list is None:
                logger.error("âŒ Failed to fetch splash list from any API")
                return False
                
            if not splash_list:
                logger.warning("âš ï¸ API returned empty splash list")
                # å³ä½¿ä¸ºç©ºä¹Ÿè§†ä¸ºæˆåŠŸ
                return True
                
            logger.info(f"ğŸ” Processing {len(splash_list)} splash items")
            
            # å¤„ç†æ¯ä¸ªå¼€å±å›¾
            for splash in splash_list:
                try:
                    # å°è¯•å¤šç§å¯èƒ½çš„ç¼©ç•¥å›¾å­—æ®µ
                    thumb_fields = ['thumb', 'image', 'url', 'image_url', 'splash_url']
                    
                    for field in thumb_fields:
                        thumb_url = splash.get(field)
                        if thumb_url:
                            result = self.download_image(thumb_url, splash)
                            if result:
                                break
                    else:
                        logger.warning(f"ğŸ” No valid thumb URL found in item: {splash}")
                except Exception as e:
                    logger.error(f"ğŸš¨ Error processing splash item: {str(e)}")
                    logger.debug(traceback.format_exc())
            
            # å³ä½¿æ²¡æœ‰ä¸‹è½½åˆ°æ–°å›¾ç‰‡ä¹Ÿè§†ä¸ºæˆåŠŸï¼ˆå¯èƒ½éƒ½æ˜¯é‡å¤å›¾ç‰‡ï¼‰
            success = True
        except Exception as e:
            logger.error(f"ğŸ”¥ Critical error: {str(e)}")
            logger.debug(traceback.format_exc())
            success = False
        finally:
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            elapsed = time.time() - self.start_time
            summary = [
                "",
                "=" * 60,
                f"ğŸš€ Splash Download Summary - {elapsed:.2f} seconds",
                f"âœ… Downloaded: {self.downloaded_count}",
                f"â© Skipped: {self.skipped_count}",
                f"âŒ Failed: {self.failed_count}",
                f"ğŸ Status: {'Success' if success else 'Failed'}",
                "=" * 60,
                ""
            ]
            
            # å°†æ‘˜è¦å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write("\n".join(summary))
            
            return success

def main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(description="Bilibili Splash Image Downloader")
    parser.add_argument("--output", default="splash", help="Output directory for images")
    parser.add_argument("--url-file", default="splash_urls.txt", help="File to record downloaded URLs")
    parser.add_argument("--log-file", default="splash.log", help="Path to log file")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument("--proxy", action="store_true", help="Use proxy for requests")
    args = parser.parse_args()
    
    # è®¾ç½®è°ƒè¯•çº§åˆ«
    if args.debug:
        root_logger.setLevel(logging.DEBUG)
        logger.debug("ğŸš§ Debug mode enabled")
    
    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = SplashDownloader(
        output_dir=args.output,
        url_file=args.url_file,
        log_file=args.log_file,
        use_proxy=args.proxy
    )
    
    # æ‰§è¡Œä¸‹è½½
    success = downloader.run()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
